{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Instrumental Variable Analysis\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "- Similar to a mediation analysis, this removes the influence of confounds from an independent variable to attempt to isolate the causal effect upon the dependent variable. \n",
    "- For further information, Causal Inference by Scott Cunningham has an excellent chapter on IV analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/joint_distribution_calculus/validation_cohort'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'\n",
    "# Specify sheet name as a string if using Excel, otherwise set to None \n",
    "sheet = 'master_list_proper_subjects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter names of columns you'd like to drop nans from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Age', 'Z_Scored_Percent_Cognitive_Improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'not'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Toronto' # The value to drop if T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Ordinal_Target_Type', 'Ordinal_Epilepsy_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regress out Covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.regression_utils import RegressOutCovariates\n",
    "# use this code block to regress out covariates. Generally better to just include as covariates in a model..\n",
    "dependent_variable_list = ['dependent_variable_column']\n",
    "regressors = ['Age', 'Sex']\n",
    "\n",
    "data_df, adjusted_dep_vars_list = RegressOutCovariates.run(df=data_df, dependent_variable_list=dependent_variable_list, covariates_list=regressors)\n",
    "print(adjusted_dep_vars_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01B - Import Directly from a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv('path/to/your/csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Evaluate Instrumental Variable\n",
    "- There are 3 assumptions to an IV analysis. This will go over them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def relevance_assumption(data_df, iv, second_var):\n",
    "    formula = f'{second_var} ~ {iv}'\n",
    "    y, X = patsy.dmatrices(formula, data=data_df, return_type='dataframe')\n",
    "    first_stage = sm.OLS(y, X).fit()\n",
    "    f_stat = first_stage.fvalue\n",
    "    print(f\"Relevance Assumption F-Statistic: {f_stat}\")\n",
    "    if f_stat > 10:\n",
    "        print(\"The instrument is acceptable (F-statistic > 10).\")\n",
    "    else:\n",
    "        print(\"The instrument is weak (F-statistic ≤ 10). Suggest not using this instrument\")\n",
    "    return f_stat\n",
    "\n",
    "def exogeneity_assumption(data_df, iv, second_var, dependent_var):\n",
    "    formula = f'{dependent_var} ~ {second_var}'\n",
    "    y, X = patsy.dmatrices(formula, data=data_df, return_type='dataframe')\n",
    "    ols_reg = sm.OLS(y, X).fit()\n",
    "    residuals = ols_reg.resid\n",
    "    r_value, p_value = pearsonr(data_df[iv], residuals)\n",
    "    print(f\"Exogeneity Assumption: Pearson correlation between residuals and IV: R = {r_value}, P = {p_value}\")\n",
    "    if p_value > 0.05:\n",
    "        print(\"Exogeneity assumption has been met (P > 0.05).\")\n",
    "    else:\n",
    "        print(\"Exogeneity assumption failed (P ≤ 0.05).\")\n",
    "    return r_value, p_value\n",
    "\n",
    "def exclusion_assumption():\n",
    "    print(\"Reminder: The exclusion assumption states that the instrumental variable should not be influenced by confounders.\")\n",
    "    print(\"This assumption must be proven through theory and domain knowledge, not through statistical tests.\")\n",
    "    \n",
    "def check_iv_assumptions(data_df, iv, second_var, dependent_var):\n",
    "    print(\"Checking Relevance Assumption:\")\n",
    "    relevance_assumption(data_df, iv, second_var)\n",
    "    \n",
    "    print(\"\\nChecking Exogeneity Assumption:\")\n",
    "    exogeneity_assumption(data_df, iv, second_var, dependent_var)\n",
    "    \n",
    "    print(\"\\nChecking Exclusion Assumption:\")\n",
    "    exclusion_assumption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumental_variable = 'Age'\n",
    "independent_variable = 'Subiculum_Grey_Matter'\n",
    "dependent_var = 'Standardized_Percent_Improvement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_iv_assumptions(data_df, iv=instrumental_variable, second_var=independent_variable, dependent_var=dependent_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Run IV analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "def iv_analysis(data_df, iv, second_var, dependent_var):\n",
    "    # 1. OLS regression using patsy formula\n",
    "    ols_formula = f'{dependent_var} ~ {second_var}'\n",
    "    y_ols, X_ols = patsy.dmatrices(ols_formula, data=data_df, return_type='dataframe')\n",
    "    ols_model = sm.OLS(y_ols, X_ols).fit()\n",
    "    \n",
    "    # 2. First stage regression (IV as the predictor for second_var)\n",
    "    first_stage_formula = f'{second_var} ~ {iv}'\n",
    "    y_first_stage, X_first_stage = patsy.dmatrices(first_stage_formula, data=data_df, return_type='dataframe')\n",
    "    first_stage_model = sm.OLS(y_first_stage, X_first_stage).fit()\n",
    "    data_df[second_var] = first_stage_model.resid\n",
    "    \n",
    "    # 3. IV (2SLS) regression using predicted values from first stage\n",
    "    iv_formula = f'{dependent_var} ~ {second_var}'\n",
    "    y_iv, X_iv = patsy.dmatrices(iv_formula, data=data_df, return_type='dataframe')\n",
    "    iv_model = sm.OLS(y_iv, X_iv).fit()\n",
    "    \n",
    "    # 4. Create DataFrame for the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Coefficient (IV)': iv_model.params,\n",
    "        'Standard Error (IV)': iv_model.bse,\n",
    "        'P-Value (IV)': iv_model.pvalues,\n",
    "        'Coefficient (OLS)': ols_model.params,\n",
    "        'Standard Error (OLS)': ols_model.bse,\n",
    "        'P-Value (OLS)': ols_model.pvalues\n",
    "    })\n",
    "    \n",
    "    # Formatting the coefficients and standard errors\n",
    "    results_df['Coefficient (IV)'] = results_df.apply(\n",
    "        lambda row: f\"{row['Coefficient (IV)']:.4f} ({row['Standard Error (IV)']:.4f})\", axis=1\n",
    "    )\n",
    "    results_df['Coefficient (OLS)'] = results_df.apply(\n",
    "        lambda row: f\"{row['Coefficient (OLS)']:.4f} ({row['Standard Error (OLS)']:.4f})\", axis=1\n",
    "    )\n",
    "    \n",
    "    # Reordering the columns for readability\n",
    "    results_df = results_df[['Coefficient (IV)', 'P-Value (IV)', 'Coefficient (OLS)', 'P-Value (OLS)']]\n",
    "    \n",
    "    # Explanation of the comparison\n",
    "    print(\"--Comparison between OLS and 2SLS coefficients--\")\n",
    "    print(\"If the OLS is unbiased, the coefficient from the IV analysis should be different from the OLS analysis. This coefficient is the 'causal effect' of the Dep var on the Indep var\")\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_analysis(data_df, iv=instrumental_variable, second_var=independent_variable, dependent_var=dependent_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
